{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        path = os.path.join(dirname, filename)\n",
    "        print(path)\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_df = pd.read_csv(\"/kaggle/input/ashrae-energy-prediction/building_metadata.csv\")\n",
    "train_df = pd.read_csv(\"/kaggle/input/ashrae-energy-prediction/train.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/ashrae-energy-prediction/test.csv\")\n",
    "weather_train_df = pd.read_csv(\"/kaggle/input/ashrae-energy-prediction/weather_train.csv\")\n",
    "weather_test_df = pd.read_csv(\"/kaggle/input/ashrae-energy-prediction/weather_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_df = reduce_mem_usage(building_df)\n",
    "train_df = reduce_mem_usage(train_df)\n",
    "weather_train_df = reduce_mem_usage(weather_train_df)\n",
    "test_df = reduce_mem_usage(test_df)\n",
    "weather_test_df = reduce_mem_usage(weather_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_merge_train_df = train_df.merge(building_df, on='building_id', how='left')\n",
    "df_train = building_merge_train_df.merge(weather_train_df, on=['site_id', 'timestamp'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = reduce_mem_usage(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_merge_test_df = test_df.merge(building_df, on='building_id', how='left')\n",
    "df_test = building_merge_test_df.merge(weather_test_df, on=['site_id', 'timestamp'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_values={'air_temperature':np.around(np.mean(df_train['air_temperature']),decimals=1),'cloud_coverage':np.around(np.mean(df_train['cloud_coverage'])),'dew_temperature':np.around(np.mean(df_train['dew_temperature']),decimals=1)}\n",
    "df_train=df_train.fillna(value=nan_values)\n",
    "nan_values={'air_temperature':np.around(np.mean(df_test['air_temperature']),decimals=1),'cloud_coverage':np.around(np.mean(df_test['cloud_coverage'])),'dew_temperature':np.around(np.mean(df_test['dew_temperature']),decimals=1)}\n",
    "df_test=df_test.fillna(value=nan_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"timestamp\"] = pd.to_datetime(df_train[\"timestamp\"])\n",
    "df_train[\"hour\"] = df_train[\"timestamp\"].dt.hour\n",
    "df_train[\"day\"] = df_train[\"timestamp\"].dt.day\n",
    "df_train[\"weekend\"] = df_train[\"timestamp\"].dt.weekday\n",
    "df_train[\"month\"] = df_train[\"timestamp\"].dt.month\n",
    "df_train = df_train.drop([\"timestamp\"], axis = 1)\n",
    "le = preprocessing.LabelEncoder()\n",
    "df_train[\"primary_use\"] = le.fit_transform(df_train[\"primary_use\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\"building_id\", \"primary_use\", \"hour\", \"day\", \"weekend\", \"month\", \"meter\"]\n",
    "\n",
    "drop_cols = [\"precip_depth_1_hr\", \"sea_level_pressure\", \"wind_direction\", \"wind_speed\",\"year_built\"]\n",
    "\n",
    "numericals = [\"square_feet\", \"air_temperature\", \"cloud_coverage\",\n",
    "              \"dew_temperature\"]\n",
    "\n",
    "feat_cols = categoricals + numericals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.log1p(df_train[\"meter_reading\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(drop_cols + [\"site_id\",\"floor_count\",\"meter_reading\"], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "kf = KFold(n_splits = num_folds, shuffle = True, random_state = 42)\n",
    "error = 0\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(df_train, target)):\n",
    "\n",
    "    print ('Training FOLD ',fold,'\\n')\n",
    "    print('Train index:','\\tfrom:',train_index.min(),'\\tto:',train_index.max())\n",
    "    print('Valid index:','\\tfrom:',val_index.min(),'\\tto:',val_index.max(),'\\n')\n",
    "    \n",
    "    train_X = df_train[feat_cols].iloc[train_index]\n",
    "    val_X = df_train[feat_cols].iloc[val_index]\n",
    "    train_y = target.iloc[train_index]\n",
    "    val_y = target.iloc[val_index]\n",
    "    lgb_train = lgb.Dataset(train_X, train_y)\n",
    "    lgb_eval = lgb.Dataset(val_X, val_y)\n",
    "    \n",
    "    params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': {'rmse'},\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.9, \n",
    "        'alpha': 0.1, \n",
    "        'lambda': 0.1\n",
    "            }\n",
    "    \n",
    "    gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=2000,\n",
    "                    categorical_feature = categoricals,\n",
    "                valid_sets=(lgb_train, lgb_eval),\n",
    "               early_stopping_rounds=20,\n",
    "               verbose_eval = 20)\n",
    "\n",
    "    y_pred = gbm.predict(val_X, num_iteration=gbm.best_iteration)\n",
    "    error += np.sqrt(mean_squared_error(y_pred, (val_y)))/num_folds\n",
    "    \n",
    "    print('\\nFold',fold,' Score: ',np.sqrt(mean_squared_error(y_pred, val_y)))\n",
    "    #print('RMSLE: ', rmsle(y_pred, val_y))\n",
    "    #print('RMSLE_2: ', np.sqrt(mean_squared_log_error(y_pred, (val_y))))\n",
    "\n",
    "    del train_X, val_X, train_y, val_y, lgb_train, lgb_eval\n",
    "    gc.collect()\n",
    "\n",
    "    print (20*'---')\n",
    "    break\n",
    "    \n",
    "print('CV error: ',error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df\n",
    "del weather_train_df\n",
    "del test_df\n",
    "del weather_test_df\n",
    "del building_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "feature_imp = pd.DataFrame(sorted(zip(gbm.feature_importance(), gbm.feature_name()),reverse = True), columns=['Value','Feature'])\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test[\"timestamp\"] = pd.to_datetime(df_test[\"timestamp\"])\n",
    "df_test[\"hour\"] = df_test[\"timestamp\"].dt.hour.astype(np.uint8)\n",
    "df_test[\"day\"] = df_test[\"timestamp\"].dt.day.astype(np.uint8)\n",
    "df_test[\"weekend\"] = df_test[\"timestamp\"].dt.weekday.astype(np.uint8)\n",
    "df_test[\"month\"] = df_test[\"timestamp\"].dt.month.astype(np.uint8)\n",
    "df_test = df_test[feat_cols]\n",
    "le = preprocessing.LabelEncoder()\n",
    "df_test[\"primary_use\"] = le.fit_transform(df_test[\"primary_use\"])\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "i=0\n",
    "res=[]\n",
    "step_size = 60000 \n",
    "for j in tqdm(range(int(np.ceil(df_test.shape[0]/60000)))):\n",
    "    res.append(np.expm1(gbm.predict(df_test.iloc[i:i+step_size])))\n",
    "    i+=step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.concatenate(res)\n",
    "sub = pd.read_csv(\"/kaggle/input/ashrae-energy-prediction/sample_submission.csv\")\n",
    "# count_row_sub = sub.shape[0]\n",
    "# sub = sub.loc[:count_row_sub*0.1-1]\n",
    "sub[\"meter_reading\"] = res\n",
    "sub.to_csv(\"submission.csv\", index = False)\n",
    "sub.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
